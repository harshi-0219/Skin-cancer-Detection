{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fedde441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tumma\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\tumma\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\tumma\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Found 11879 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "372/372 [==============================] - 3017s 8s/step\n",
      "63/63 [==============================] - 525s 8s/step\n",
      "372/372 [==============================] - 729s 2s/step\n",
      "63/63 [==============================] - 217s 3s/step\n"
     ]
    }
   ],
   "source": [
    "#VGGNet,resnet\n",
    "import numpy as np\n",
    "from keras.applications import VGG16, ResNet50\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define image dimensions\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "# Load VGG16 pre-trained model\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "# Load ResNet50 pre-trained model\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "# Define data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "# Set your training and testing directories\n",
    "train_dir = \"C:\\\\Users\\\\tumma\\\\Desktop\\\\skincancerdetection\\\\train\"\n",
    "test_dir = \"C:\\\\Users\\\\tumma\\\\Desktop\\\\skincancerdetection\\\\test\"\n",
    "\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Generate training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,  # This means our generator will only yield batches of data, no labels\n",
    "    shuffle=False)  # Important to keep the same order as our labels\n",
    "\n",
    "# Generate testing data\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    shuffle=False)\n",
    "\n",
    "# Extract features using VGG16\n",
    "X_train_features = vgg_model.predict(train_generator, verbose=1)\n",
    "X_test_features = vgg_model.predict(test_generator, verbose=1)\n",
    "\n",
    "# Extract features using ResNet50\n",
    "resnet_X_train_features = resnet_model.predict(train_generator, verbose=1)\n",
    "resnet_X_test_features = resnet_model.predict(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2268d002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 128)               327808    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 338210 (1.29 MB)\n",
      "Trainable params: 338210 (1.29 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "372/372 [==============================] - 12s 15ms/step - loss: 0.7046 - accuracy: 0.5220 - val_loss: 0.6929 - val_accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "372/372 [==============================] - 4s 10ms/step - loss: 0.6596 - accuracy: 0.6005 - val_loss: 0.6060 - val_accuracy: 0.6970\n",
      "Epoch 3/20\n",
      "372/372 [==============================] - 3s 7ms/step - loss: 0.6195 - accuracy: 0.6489 - val_loss: 0.6345 - val_accuracy: 0.6260\n",
      "Epoch 4/20\n",
      "372/372 [==============================] - 21s 57ms/step - loss: 0.6066 - accuracy: 0.6649 - val_loss: 0.5757 - val_accuracy: 0.7050\n",
      "Epoch 5/20\n",
      "372/372 [==============================] - 2s 6ms/step - loss: 0.6101 - accuracy: 0.6581 - val_loss: 0.6144 - val_accuracy: 0.6500\n",
      "Epoch 6/20\n",
      "372/372 [==============================] - 2s 6ms/step - loss: 0.6097 - accuracy: 0.6591 - val_loss: 0.6028 - val_accuracy: 0.6770\n",
      "Epoch 7/20\n",
      "372/372 [==============================] - 2s 6ms/step - loss: 0.5973 - accuracy: 0.6719 - val_loss: 0.6318 - val_accuracy: 0.6290\n",
      "Epoch 8/20\n",
      "372/372 [==============================] - 2s 6ms/step - loss: 0.5952 - accuracy: 0.6730 - val_loss: 0.5515 - val_accuracy: 0.7330\n",
      "Epoch 9/20\n",
      "372/372 [==============================] - 2s 6ms/step - loss: 0.5955 - accuracy: 0.6752 - val_loss: 0.5574 - val_accuracy: 0.7205\n",
      "Epoch 10/20\n",
      "372/372 [==============================] - 2s 6ms/step - loss: 0.5923 - accuracy: 0.6746 - val_loss: 0.6040 - val_accuracy: 0.6605\n",
      "Epoch 11/20\n",
      "372/372 [==============================] - 2s 6ms/step - loss: 0.5884 - accuracy: 0.6793 - val_loss: 0.6310 - val_accuracy: 0.6265\n",
      "Epoch 12/20\n",
      "372/372 [==============================] - 2s 6ms/step - loss: 0.5934 - accuracy: 0.6763 - val_loss: 0.6172 - val_accuracy: 0.6455\n",
      "Epoch 13/20\n",
      "372/372 [==============================] - 2s 6ms/step - loss: 0.5902 - accuracy: 0.6782 - val_loss: 0.5328 - val_accuracy: 0.7640\n",
      "Epoch 14/20\n",
      "372/372 [==============================] - 2s 6ms/step - loss: 0.5847 - accuracy: 0.6841 - val_loss: 0.5716 - val_accuracy: 0.7035\n",
      "Epoch 15/20\n",
      "372/372 [==============================] - 2s 6ms/step - loss: 0.5832 - accuracy: 0.6852 - val_loss: 0.5596 - val_accuracy: 0.7235\n",
      "Epoch 16/20\n",
      "372/372 [==============================] - 2s 7ms/step - loss: 0.5807 - accuracy: 0.6904 - val_loss: 0.5461 - val_accuracy: 0.7355\n",
      "Epoch 17/20\n",
      "372/372 [==============================] - 3s 7ms/step - loss: 0.5811 - accuracy: 0.6861 - val_loss: 0.5415 - val_accuracy: 0.7360\n",
      "Epoch 18/20\n",
      "372/372 [==============================] - 2s 7ms/step - loss: 0.5796 - accuracy: 0.6868 - val_loss: 0.5453 - val_accuracy: 0.7340\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7640\n",
      "Test accuracy: 0.7639999985694885\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import GlobalAveragePooling2D, Concatenate\n",
    "from keras.callbacks import EarlyStopping\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Apply Global Average Pooling to the feature maps\n",
    "vgg_pooled_features = GlobalAveragePooling2D()(X_train_features)\n",
    "resnet_pooled_features = GlobalAveragePooling2D()(resnet_X_train_features)\n",
    "\n",
    "# Concatenate the pooled feature maps\n",
    "concatenated_X_train_features = Concatenate(axis=1)([vgg_pooled_features, resnet_pooled_features])\n",
    "\n",
    "# Apply the same operations to the test data\n",
    "vgg_pooled_features_test = GlobalAveragePooling2D()(X_test_features)\n",
    "resnet_pooled_features_test = GlobalAveragePooling2D()(resnet_X_test_features)\n",
    "concatenated_X_test_features = Concatenate(axis=1)([vgg_pooled_features_test, resnet_pooled_features_test])\n",
    "\n",
    "# Get the actual class labels for training and testing data\n",
    "y_train = train_generator.classes\n",
    "y_test = test_generator.classes\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(concatenated_X_train_features.shape[1],)),  \n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(concatenated_X_train_features, y_train, epochs=20, batch_size=32, validation_data=(concatenated_X_test_features, y_test), callbacks=[early_stop])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(concatenated_X_test_features, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b341d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f267aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Predicted class: Malignant\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Define the function to preprocess the image\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(img_width, img_height))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Expand dimensions to match batch size\n",
    "    img_array /= 255.  # Normalize pixel values\n",
    "    return img_array\n",
    "\n",
    "# Define the function to predict the class of the image\n",
    "def predict_image_class(img_path):\n",
    "    preprocessed_img = preprocess_image(img_path)\n",
    "    # Extract features using VGG16\n",
    "    vgg_features = vgg_model.predict(preprocessed_img)\n",
    "    # Extract features using ResNet50\n",
    "    resnet_features = resnet_model.predict(preprocessed_img)\n",
    "    # Apply Global Average Pooling to the feature maps\n",
    "    vgg_pooled_features = np.mean(vgg_features, axis=(1, 2))\n",
    "    resnet_pooled_features = np.mean(resnet_features, axis=(1, 2))\n",
    "    # Concatenate the pooled feature maps\n",
    "    concatenated_features = np.concatenate([vgg_pooled_features, resnet_pooled_features], axis=1)\n",
    "    # Make prediction using the concatenated features\n",
    "    predictions = model.predict(concatenated_features)\n",
    "    # Interpret predictions\n",
    "    if np.argmax(predictions) == 0:\n",
    "        return \"Benign\"\n",
    "    else:\n",
    "        return \"Malignant\"\n",
    "\n",
    "# Test the function on an example image\n",
    "test_image_path = \"C:\\\\Users\\\\tumma\\\\Desktop\\\\skincancerdetection\\\\test\\\\Malignant\\\\5660.jpg\"  # Provide the path to your test image\n",
    "predicted_class = predict_image_class(test_image_path)\n",
    "print(\"Predicted class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348ef62a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
