{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5e57bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tumma\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\tumma\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\tumma\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
      "17225924/17225924 [==============================] - 3s 0us/step\n",
      "Found 11879 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "372/372 [==============================] - 1500s 4s/step\n",
      "63/63 [==============================] - 315s 5s/step\n",
      "372/372 [==============================] - 140s 376ms/step\n",
      "63/63 [==============================] - 26s 403ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.applications import VGG16, MobileNet\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define image dimensions\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "# Load VGG16 pre-trained model\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "# Load MobileNet pre-trained model\n",
    "mobilenet_model = MobileNet(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "# Define data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "# Set your training and testing directories\n",
    "train_dir = \"C:\\\\Users\\\\tumma\\\\Desktop\\\\skincancerdetection\\\\train\"\n",
    "test_dir = \"C:\\\\Users\\\\tumma\\\\Desktop\\\\skincancerdetection\\\\test\"\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Generate training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,  # This means our generator will only yield batches of data, no labels\n",
    "    shuffle=False)  # Important to keep the same order as our labels\n",
    "\n",
    "# Generate testing data\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    shuffle=False)\n",
    "\n",
    "# Extract features using VGG16\n",
    "X_train_features = vgg_model.predict(train_generator, verbose=1)\n",
    "X_test_features = vgg_model.predict(test_generator, verbose=1)\n",
    "\n",
    "# Extract features using MobileNet\n",
    "mobilenet_X_train_features = mobilenet_model.predict(train_generator, verbose=1)\n",
    "mobilenet_X_test_features = mobilenet_model.predict(test_generator, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15d67716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_75 (Dense)            (None, 128)               196736    \n",
      "                                                                 \n",
      " dropout_57 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_76 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_58 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_59 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 207138 (809.13 KB)\n",
      "Trainable params: 207138 (809.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "372/372 [==============================] - 2s 4ms/step - loss: 0.4487 - accuracy: 0.7983 - val_loss: 0.3486 - val_accuracy: 0.8535\n",
      "Epoch 2/5\n",
      "372/372 [==============================] - 2s 4ms/step - loss: 0.3202 - accuracy: 0.8749 - val_loss: 0.4133 - val_accuracy: 0.8175\n",
      "Epoch 3/5\n",
      "372/372 [==============================] - 2s 4ms/step - loss: 0.2897 - accuracy: 0.8842 - val_loss: 0.3258 - val_accuracy: 0.8595\n",
      "Epoch 4/5\n",
      "372/372 [==============================] - 1s 4ms/step - loss: 0.2802 - accuracy: 0.8876 - val_loss: 0.3597 - val_accuracy: 0.8560\n",
      "Epoch 5/5\n",
      "372/372 [==============================] - 1s 4ms/step - loss: 0.2629 - accuracy: 0.8926 - val_loss: 0.3292 - val_accuracy: 0.8615\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3292 - accuracy: 0.8615\n",
      "Test accuracy: 0.8615000247955322\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Concatenate\n",
    "\n",
    "# Apply Global Average Pooling to the feature maps\n",
    "vgg_pooled_features = GlobalAveragePooling2D()(X_train_features)\n",
    "mobilenet_pooled_features = GlobalAveragePooling2D()(mobilenet_X_train_features)\n",
    "\n",
    "# Concatenate the pooled feature maps\n",
    "concatenated_X_train_features = Concatenate(axis=1)([vgg_pooled_features, mobilenet_pooled_features])\n",
    "\n",
    "# Apply the same operations to the test data\n",
    "vgg_pooled_features_test = GlobalAveragePooling2D()(X_test_features)\n",
    "mobilenet_pooled_features_test = GlobalAveragePooling2D()(mobilenet_X_test_features)\n",
    "concatenated_X_test_features = Concatenate(axis=1)([vgg_pooled_features_test, mobilenet_pooled_features_test])\n",
    "y_train = train_generator.classes\n",
    "# Get the actual class labels for testing data\n",
    "y_test = test_generator.classes\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(concatenated_X_train_features.shape[1],)),  \n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "from keras.callbacks import EarlyStopping\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "# Train the model\n",
    "history = model.fit(concatenated_X_train_features, y_train, epochs=5, batch_size=32, validation_data=(concatenated_X_test_features, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(concatenated_X_test_features, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8a1cba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 807ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 729ms/step\n",
      "Predicted class: Benign\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Define the function to preprocess the image\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(img_width, img_height))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Expand dimensions to match batch size\n",
    "    img_array /= 255.  # Normalize pixel values\n",
    "    return img_array\n",
    "\n",
    "# Define the function to predict the class of the image\n",
    "def predict_image_class(img_path):\n",
    "    preprocessed_img = preprocess_image(img_path)\n",
    "    # Extract features using both VGG16 and MobileNet\n",
    "    vgg_features = vgg_model.predict(preprocessed_img)\n",
    "    mobilenet_features = mobilenet_model.predict(preprocessed_img)\n",
    "    # Apply Global Average Pooling to the feature maps\n",
    "    vgg_pooled_features = np.mean(vgg_features, axis=(1, 2))\n",
    "    mobilenet_pooled_features = np.mean(mobilenet_features, axis=(1, 2))\n",
    "    # Concatenate the pooled feature maps\n",
    "    concatenated_features = np.concatenate([vgg_pooled_features, mobilenet_pooled_features], axis=1)\n",
    "    predictions = model.predict(concatenated_features)\n",
    "    # Interpret predictions\n",
    "    if predictions[0][0] > predictions[0][1]:\n",
    "        return \"Benign\"\n",
    "    else:\n",
    "        return \"Malignant\"\n",
    "\n",
    "# Test the function on an example image\n",
    "test_image_path = \"C:\\\\Users\\\\tumma\\\\Desktop\\\\skincancerdetection\\\\test\\\\Benign\\\\6310.jpg\"  # Provide the path to your test image\n",
    "predicted_class = predict_image_class(test_image_path)\n",
    "print(\"Predicted class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ca5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
